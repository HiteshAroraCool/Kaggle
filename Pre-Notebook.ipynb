{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30407,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/bcscuwe1/notes-ml-eda-y-profilling-boosting-cheatsheet?scriptVersionId=178938831\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"<a id='top'></a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\"></div>\n\n## <div style=\"text-align: left; background-color:#1192AA; font-family: Trebuchet MS; color: Yellow; padding: 13px; line-height:0.9;border-radius:2px; margin-bottom: 0em; text-align: center; font-size: 25px\">TABLE OF CONTENTS</div>\n\n* **[Personal Notes](#Notes)**\n\n* **[Markdown Customization](#Markdown-Customization)**\n\n* **[Chart's Customization](#Chart's-Customization)**\n\n* **[Y-data Profiling Report](#Y-data-Profiling)**\n\n* **[Metrics and scoring: quantifying the quality of predictions (Link)](https://scikit-learn.org/stable/modules/model_evaluation.html)**\n\n\n* **[Repeated IMPORT](#Imports)**\n    * &nbsp; [EDA](#EDA)\n    * &nbsp; [ML](#ML)\n        * [SKlearn: API Web-site](https://scikit-learn.org/stable/modules/classes.html)\n    * &nbsp; [Read Data and Null Chart](#Read-Data-and-Null-Chart)\n        * [Column Limit-500](#Max-Columns-Limit-500)\n        * [User Input Column Limit](#User-Input-Column-Limit)\n        * [Table of Columns with Missing Values: Count and Percentage Analysis](#Table-of-Columns-with-Missing-Values:-Count-and-Percentage-Analysis)\n    \n\n* **[Data wrangling](#Data-wrangling)**\n    * [Dict of All Column's Type](#Dict-of-All-Column's-Type)\n    * [New Columns Categorized Based upon anther Column](#New-Columns-Categorized-Based-upon-anther-Column)\n    * [Unique Values](#Unique-Values)\n    * [Values and Percentage of a column](#Function-to-print-values-and-percentage-of-a-column-side-by-side)\n    * [Quantile](#Get-95%-of-data)\n    * [Remove conjunctions, preposition, number, nouns](#Remove-conjunctions,-preposition,-number,-nouns)\n    * [Gender List](#Gender-list)\n    * [Multicollinearity](#Multicollinearity)\n    \n    \n* **[Map](#Map)**\n\n* **[Models](#Models)**\n    * [Lazy Predict:Runs every ML Model](#Lazy-Predict)\n    * [Over/Under-fiting Function](#Over/Under-fiting-Function)\n    * [Regressions](#Regressions-(ChartGPU))\n    * [Hyperparameter Regression](#Hyperparameter-Regression-(ChatGPU))\n    * [Hyperparameter Classifiers](#Hyperparameter-Classifiers)\n    * [Hyperparameter GridSearchCV](#Hyperparameters-GridSearchCV)\n    * [K-means elbow method and scatterplot](#Interactive-Function-for-Elbow-Method-Plot-and-User-Defined-Clusters-in-K-means-Scatterplot)\n    * [SKlearn Decision Trees (DTs)](#Display-Plot-Tree-of-SKlearn-Decision-Trees-(DTs))\n\n\n* **[Evaluation Matrix](#Evaluation-Matrix)**\n    * [Classification](#Classification)\n    * [Confusion Matrix Function](#Confusion-Matrix-Function)\n\n\n* **[Boosting](#Boosting)**\n    * [ADABoost](#ADABoost)","metadata":{}},{"cell_type":"markdown","source":"# **Notes**\n\n* Going though each model with different parameter is time consuming so we can use sklearn.model_selection.GridSearchCV\n* new_data.loc[:,new_data.columns!='Survived'] and X = new_data.loc[:new_data.columns!='Survived'] Without the comma, the code treats new_data.columns != 'Survived' as the row indexing condition, rather than a column indexing condition.\n* df[~df['Survived']==0] the negation operator ~ has higher precedence than the comparison operator ==. ~df['Survived'] is evaluated first, resulting in a boolean Series\n* encoding='cp1252' is required for some file\n* outliers may be due to measurement errors or other anomalies and can distort the analysis, in which case it may be appropriate to remove them.\n* Convert object type columns to the appropriate data type based on their values in the column.\n* %matplotlib inline: it make the use of plt.show() redundent.\n* semicolon will stop unwanted titles.\n* auto precentage: autopct='%1.1f%%' in pie chart\n* use data.info() it's better(mine)\n* kaggle can dowload file from drive straight: gspread(erorr) though API\n* sns.despine to remove borders\n* The issue with your code is that you're using square brackets [] to call the pd.DataFrame() constructor, which creates a TypeError because you cannot subscript the type pd.DataFrame. Instead, you should use parentheses () to call the constructor with the dictionary containing the columns you want to combine.\n* display only first few columns df_top_20.iloc[:,:number]\n* sns.barplot(data=sales, y='Total', x=sales['Product line'], hue=sales['Gender'], palette={'Female':'#ff70a6', 'Male':'#70d6ff'}) In this code, palette argument is used to specify the colors for each category in the hue variable. The keys in the dictionary correspond to the category names and the values correspond to the colors assigned to those categories.\n* Create venn diagram venn2(title_sets[idx], set_colors=c, set_labels = (' ', ' '))\n* The ValueError you encountered is related to the target variable (y) in the RandomForestClassifier. It indicates that the classifier is expecting a categorical or discrete target variable for classification, but it received a continuous target variable instead. In classification tasks, the target variable should represent the class or category labels that the model aims to predict. It should not be a continuous numerical value.\n* count, plt.pie, stripplot(str values)/swarmplot(small dataset) don't require numeric column but cat,box,bar,violin,pointplot require numeric values in column if you need these plot use one hot encoding first.\n* A basic methodology for qualitative data is impute using mode. A basic methodology for quantitative data is impute using mean, median, or mean + randomized standard deviation.\n* The error message you're encountering indicates that the scoring function you're using **(likely accuracy_score) is not compatible with continuous target values**. The scoring function expects binary or multiclass labels, not continuous numerical values. You might be using the wrong scoring function for your regression problem. In regression tasks, accuracy is not a suitable metric. Instead, you should use metrics like Mean Absolute Error (MAE), Mean Squared Error (MSE), or R-squared (coefficient of determination) to evaluate the performance of regression models. If you're performing regression tasks, make sure to use appropriate regression evaluation metrics, and if you're performing classification tasks, ensure that you're using the correct classification scoring functions.","metadata":{}},{"cell_type":"markdown","source":"## <div style=\"text-align: left; background-color:#1192AA; font-family: Trebuchet MS; color: #FFFF00; padding: 13px; line-height:0.9;border-radius:2px; margin-bottom: 0em; text-align: center; font-size: 25px\">Markdown Customization</div>","metadata":{}},{"cell_type":"markdown","source":"***The code in this Notebook is be hidden, but you can open it by clicking on the \"Show code\" button.***","metadata":{}},{"cell_type":"code","source":"'''courtasy of https://www.kaggle.com/code/marshuu/dog-breeds-linear-regression-analysis/notebook'''\nfrom IPython.display import display\nfrom IPython.display import HTML\nimport IPython.core.display as di\n\n# This line will hide code by default when the notebook is exported as HTML\n#di.display_html('<script>jQuery(function() {if (jQuery(\"body.notebook_app\").length == 0) { jQuery(\".input_area\").toggle(); jQuery(\".prompt\").toggle();}});</script>', raw=True)\n\n# \"When the toggle is clicked, it reveals hidden code if it's currently hidden, and vice versa.\"\n# This line will add a button to toggle visibility of code blocks, for use with the HTML export version\ndi.display_html('''<button onclick=\"jQuery('.input_area').toggle(); jQuery('.prompt').toggle();\">Hide code/Show Code</button>''', raw=True)","metadata":{"execution":{"iopub.status.busy":"2023-06-20T05:58:16.00287Z","iopub.execute_input":"2023-06-20T05:58:16.003565Z","iopub.status.idle":"2023-06-20T05:58:16.01955Z","shell.execute_reply.started":"2023-06-20T05:58:16.00351Z","shell.execute_reply":"2023-06-20T05:58:16.017138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ChartGPT version\nfrom IPython.display import display, HTML\n\n# Function to toggle visibility of code blocks\ndef toggle_code():\n    display(HTML('''\n        <script>\n            function toggle_code_cells() {\n                var code_cells = document.querySelectorAll('.input');\n                code_cells.forEach(function(cell) {\n                    if (cell.style.display === 'none') {\n                        cell.style.display = 'block';\n                    } else {\n                        cell.style.display = 'none';\n                    }\n                });\n            }\n        </script>\n        <button onclick=\"toggle_code_cells()\">Hide code/Show Code</button>\n    '''))\n\n# Call the function to display the toggle button\ntoggle_code()\n","metadata":{"execution":{"iopub.status.busy":"2023-06-20T05:58:30.370657Z","iopub.execute_input":"2023-06-20T05:58:30.37117Z","iopub.status.idle":"2023-06-20T05:58:30.380554Z","shell.execute_reply.started":"2023-06-20T05:58:30.371124Z","shell.execute_reply":"2023-06-20T05:58:30.379523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <div style=\"text-align: left; background-color:#1192AA; font-family: Trebuchet MS; color: Yellow; padding: 13px; line-height:0.9;border-radius:2px; margin-bottom: 0em; text-align: center; font-size: 25px\">Chart's Customization</div>","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline \nlist = plt.style.available\nfor x in list:\n    plt.style.use(x)\n    plt.xlabel('X')\n    plt.ylabel(x, loc='top', rotation=0)\n    plt.show()\n    print('\\n')","metadata":{"execution":{"iopub.status.busy":"2023-06-28T20:29:08.647081Z","iopub.execute_input":"2023-06-28T20:29:08.648169Z","iopub.status.idle":"2023-06-28T20:29:13.689757Z","shell.execute_reply.started":"2023-06-28T20:29:08.648123Z","shell.execute_reply":"2023-06-28T20:29:13.688258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (10,5)\nplt.xticks(rotation = 45) ## label rotation at 45\nplt.title('Title')\nplt.ylabel('Y-label')\nplt.xlabel('X-label')\nsns.set_style('ticks', {'ytick.major.size': 7, 'xtick.major.size': 0})\nplt.grid(axis='y', linestyle='--')","metadata":{"execution":{"iopub.status.busy":"2023-05-03T12:49:52.6391Z","iopub.status.idle":"2023-05-03T12:49:52.639651Z","shell.execute_reply.started":"2023-05-03T12:49:52.639341Z","shell.execute_reply":"2023-05-03T12:49:52.639365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, ax = plt.subplots(figsize=(6, 5))\nax.legend(ncol=2, loc=\"lower right\", frameon=True)\nax.set(xlim=(0, 24), ylabel=\"\",\n       xlabel=\"X\")\nsns.despine(left=True, bottom=True)","metadata":{"execution":{"iopub.status.busy":"2023-05-03T12:49:52.641442Z","iopub.status.idle":"2023-05-03T12:49:52.641852Z","shell.execute_reply.started":"2023-05-03T12:49:52.641647Z","shell.execute_reply":"2023-05-03T12:49:52.641669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(plt.rcParams.keys())","metadata":{"execution":{"iopub.status.busy":"2023-05-03T12:49:52.644548Z","iopub.status.idle":"2023-05-03T12:49:52.645527Z","shell.execute_reply.started":"2023-05-03T12:49:52.644932Z","shell.execute_reply":"2023-05-03T12:49:52.644973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# show values on bar pole\nax = sns.countplot(x='Survived', data)\nplt.xticks(np.arange(2), ['label of bar 1', 'label of bar 2'])\nplt.title('Title', fontsize=14)\nplt.xlabel('label')\nplt.ylabel('label')\n\nlabels = 'data valuse of label of bar'\nfor i, v in enumerate(lables):\n    ax.text(i, v-40, str(v), horizaontalalignment= 'center', size=14, fontweight='bold', color='w')","metadata":{"execution":{"iopub.status.busy":"2023-05-03T12:49:52.647579Z","iopub.status.idle":"2023-05-03T12:49:52.648282Z","shell.execute_reply.started":"2023-05-03T12:49:52.647937Z","shell.execute_reply":"2023-05-03T12:49:52.647975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_countplots(df, num_cols, palette):\n    num_rows = (len(df_test.columns)) // num_cols\n    fig, axes = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(18, 4*num_rows))\n    sns.set(font_scale=1.2, style='white')\n\n    for i, col_name in enumerate(df_test.columns):\n        #if (col_name != 'is_generated') or (col_name != target_col):\n        ax = axes[(i-1) // num_cols, (i-1) % num_cols]\n        sns.countplot(data=df, x=col_name, ax=ax, palette=palette)\n        ax.set_title(f'{col_name.title()}', fontsize=18)\n        ax.set_xlabel(col_name.title(), fontsize=14)\n        ax.tick_params(axis='both', which='major', labelsize=12)\n        sns.despine()\n\n    plt.tight_layout()\n    plt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"my_palette = ['#1192AA','#3E2756']\nplot_countplots(pd.concat([df_train, original], axis=0).reset_index(drop=True), 4, my_palette)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <div style=\"text-align: left; background-color:#1192AA; font-family: Trebuchet MS; color: Yellow; padding: 13px; line-height:0.9;border-radius:2px; margin-bottom: 0em; text-align: center; font-size: 25px\">Y-data Profiling</div>","metadata":{}},{"cell_type":"code","source":"#https://ydata-profiling.ydata.ai/docs/master/index.html\n# Standard Library Imports\n\nfrom pathlib import Path\n\n# Installed packages\nfrom ipywidgets import widgets\n#!{sys.executable} -m pip install -U ydata-profiling[notebook]\n!jupyter nbextension enable --py widgetsnbextension\n# Our package\nfrom ydata_profiling import ProfileReport\nfrom ydata_profiling.utils.cache import cache_file\n\n\n# Generate the Profiling Report\nprofile = ProfileReport(\n    data, title=\"Dataset\", html={\"style\": {\"full_width\": True}}, sort=None\n)\n\n# The Notebook Widgets Interface\nprofile.to_widgets()\n\n#profile.to_file(\"your_report.html\")\n\n# Or use the HTML report in an iframe\nprofile\n#profile.to_notebook_iframe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas_profiling\n\n# Load data\ndf = pd.read_csv(\"data.csv\")\n\n# Generate report\nprofile = pandas_profiling.ProfileReport(df)\n\n# Display report in notebook\nprofile.to_widgets()\n\n# Save report to HTML file\nprofile.to_file(\"my_report.html\")\n\n# Get HTML link from notebook interface\n# Right-click on the HTML file in the file browser and select \"Copy Path\"\n# Then paste the path into a Markdown cell in your notebook and surround it with an HTML link tag\n# Example: <a href=\"/files/my_report.html\" target=\"_blank\">Open report</a>\n","metadata":{"execution":{"iopub.status.busy":"2023-06-20T05:04:20.312626Z","iopub.status.idle":"2023-06-20T05:04:20.313253Z","shell.execute_reply.started":"2023-06-20T05:04:20.312865Z","shell.execute_reply":"2023-06-20T05:04:20.312891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <div style=\"text-align: left; background-color:#1192AA; font-family: Trebuchet MS; color: Yellow; padding: 13px; line-height:0.9;border-radius:2px; margin-bottom: 0em; text-align: center; font-size: 25px\">Imports</div>","metadata":{}},{"cell_type":"markdown","source":"## EDA","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline \n# magic command that allows the plots to be displayed directly in the Jupyter Notebook.\nimport seaborn as sns # data visualization library based on matplotlib that provides a high-level interface for creating attractive and informative statistical graphics\nimport sklearn # A machine learning library in Python that provides a range of tools for data analysis, modeling, and evaluation.\nimport scipy # A library for scientific computing and technical computing, often used for statistical analysis and optimization.\nimport datetime # Library for working with dates and times in Python.\nimport folium # Python library that allows users to create interactive maps and visualizations from geospatial data.\nimport plotly.graph_objs as go\nfrom tabulate import tabulate  #function can be used to convert data, such as lists or pandas DataFrames, into a formatted table\nfrom tqdm import tqdm # put tqdm in front of range() #library allows you to visualize the progress of your code execution.\n\npd.set_option('display.max_columns',500) \n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Chart for Null values in data\ndef nullchart(df):\n    null_appDF = pd.DataFrame((df.isnull().sum())*100/df.shape[0]).reset_index()\n    null_appDF.columns = ['Column Name', 'Null Values Percentage']\n    fig = plt.figure(figsize=(18,6))\n    ax = sns.pointplot(x=\"Column Name\",y=\"Null Values Percentage\",data=null_appDF,color='blue')\n    plt.xticks(rotation =90,fontsize =7)\n    ax.axhline(40, ls='--',color='red')\n    plt.title(\"Percentage of Missing values in data\")\n    plt.ylabel(\"Null Values PERCENTAGE\")\n    plt.xlabel(\"COLUMNS\")\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-06-15T19:54:51.025811Z","iopub.execute_input":"2023-06-15T19:54:51.026548Z","iopub.status.idle":"2023-06-15T19:54:52.270951Z","shell.execute_reply.started":"2023-06-15T19:54:51.026483Z","shell.execute_reply":"2023-06-15T19:54:52.269587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ML","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\nfrom sklearn.neighbors import LocalOutlierFactor, KNeighborsClassifier\nfrom sklearn.preprocessing import MinMaxScaler, LabelEncoder, StandardScaler, RobustScaler, OneHotEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\n\n# Common Model Algorithms\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\n\n# Common Model Helpers\nfrom sklearn import feature_selection, model_selection, metrics","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Read Data and Null Chart","metadata":{}},{"cell_type":"markdown","source":"### Max-Columns-Limit-500","metadata":{}},{"cell_type":"code","source":"pd.set_option('display.max_columns',500) \n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Chart for Null values in data\ndef nullchart(df):\n    null_appDF = pd.DataFrame((df.isnull().sum())*100/df.shape[0]).reset_index()\n    null_appDF.columns = ['Column Name', 'Null Values Percentage']\n    fig = plt.figure(figsize=(18,6))\n    ax = sns.pointplot(x=\"Column Name\",y=\"Null Values Percentage\",data=null_appDF,color='blue')\n    plt.xticks(rotation =90,fontsize =7)\n    ax.axhline(40, ls='--',color='red')\n    plt.title(\"Percentage of Missing values in data\")\n    plt.ylabel(\"Null Values PERCENTAGE\")\n    plt.xlabel(\"COLUMNS\")\n    plt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### User Input Column Limit ","metadata":{}},{"cell_type":"code","source":"pd.set_option('display.max_columns',int(input('Maxium number of colunms:')))\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Chart for Null values in data\ndef nullchart(df):\n    null_appDF = pd.DataFrame((df.isnull().sum())*100/df.shape[0]).reset_index()\n    null_appDF.columns = ['Column Name', 'Null Values Percentage']\n    fig = plt.figure(figsize=(18,6))\n    ax = sns.pointplot(x=\"Column Name\",y=\"Null Values Percentage\",data=null_appDF,color='blue')\n    plt.xticks(rotation =90,fontsize =7)\n    ax.axhline(40, ls='--',color='red')\n    plt.title(\"Percentage of Missing values in data\")\n    plt.ylabel(\"Null Values PERCENTAGE\")\n    plt.xlabel(\"COLUMNS\")\n    plt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Table of Columns with Missing Values: Count and Percentage Analysis","metadata":{}},{"cell_type":"code","source":"# credit: https://www.kaggle.com/willkoehrsen/start-here-a-gentle-introduction. \n# One of the best notebooks on getting started with a ML problem.\n\ndef missing_values_table(df):\n        # Total missing values\n        mis_val = df.isnull().sum()\n        \n        # Percentage of missing values\n        mis_val_percent = 100 * df.isnull().sum() / len(df)\n        \n        # Make a table with the results\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        \n        # Rename the columns\n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n        \n        # Sort the table by percentage of missing descending\n        mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n        \n        # Print some summary information\n        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n              \" columns that have missing values.\")\n        \n        # Return the dataframe with missing information\n        return mis_val_table_ren_columns\ntrain_missing= missing_values_table(train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <div style=\"text-align: left; background-color:#1192AA; font-family: Trebuchet MS; color: Yellow; padding: 13px; line-height:0.9;border-radius:2px; margin-bottom: 0em; text-align: center; font-size: 25px\">Data wrangling</div>","metadata":{}},{"cell_type":"markdown","source":"## Outliers","metadata":{}},{"cell_type":"code","source":"# outliers replaced with mean\nQ1 = data.quantile(0.25)\nQ3 = data.quantile(0.75)\nIOR = Q3 - Q1\nlower_limit = Q1 - 1.5*IOR\nupper_limit = Q3 + 1.5*IOR\ndf = data[(data>lower_limit) & (data<upper_limit)]\ndf\n\n# outliers replaced with mean\nQ1 = data.quantile(0.25)\nQ3 = data.quantile(0.75)\nIOR = Q3 - Q1\nlower_limit = Q1 - 1.5*IOR\nupper_limit = Q3 + 1.5*IOR\ndf = data.apply(lambda x: np.where((x < lower_limit[x.name]) | (x > upper_limit[x.name]), round(x.median(), 2), x))\ndf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dict of All Column's Type","metadata":{}},{"cell_type":"code","source":"data_types_dict = {}\nfor column in data.columns:\n    if data[column].dtype == 'object':\n        data_types_dict['object'] = data_types_dict.get('object', []) + [column]\n    elif data[column].dtype == 'int64':\n        data_types_dict['int64'] = data_types_dict.get('int64', []) + [column]\n    elif data[column].dtype == 'float64':\n        data_types_dict['float64'] = data_types_dict.get('float64', []) + [column]\n    elif data[column].dtype == 'bool':\n        data_types_dict['bool'] = data_types_dict.get('bool', []) + [column]\n    elif data[column].dtype == 'datetime64[ns]':\n        data_types_dict['datetime64[ns]'] = data_types_dict.get('datetime64[ns]', []) + [column]\n    elif data[column].dtype == 'timedelta64[ns]':\n        data_types_dict['timedelta64[ns]'] = data_types_dict.get('timedelta64[ns]', []) + [column]\n    elif data[column].dtype == 'category':\n        data_types_dict['category'] = data_types_dict.get('category', []) + [column]\n    else:\n        data_types_dict['others'] = data_types_dict.get('others', []) + [column]\nprint(data_types_dict)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## New Columns Categorized Based upon anther Column","metadata":{}},{"cell_type":"code","source":"# making columns from categories in another column\nlabel_income = {0: 'Middle', 1: 'High', 2: 'Low', 3: 'High', 4: 'Low'}\nlabel_spend = {0: 'Medium', 1: 'Low', 2: 'Low', 3: 'High', 4: 'High'}\n\nX_df['Income'] = X_df['labels'].map(label_income)\nX_df['Spending'] = X_df['labels'].map(label_spend)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Unique Values","metadata":{}},{"cell_type":"code","source":"#String Table format (better)\nfrom tabulate import tabulate\n\ndef unique_columns(data):\n    \"\"\"\n    Print number of unique values for each column in [data]\n    \"\"\"\n    table = []\n    for column in data.columns:\n        table.append([column, len(data[column].unique()), data[column].dtype])\n    \n    print(tabulate(table, headers=[\"Column\", \"Number of Unique Values\", \"Dtype\"], tablefmt=\"presto\"))","metadata":{"execution":{"iopub.status.busy":"2023-06-14T04:20:25.328204Z","iopub.execute_input":"2023-06-14T04:20:25.328592Z","iopub.status.idle":"2023-06-14T04:20:25.335657Z","shell.execute_reply.started":"2023-06-14T04:20:25.328556Z","shell.execute_reply":"2023-06-14T04:20:25.334024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Table Format\ndef unique_col_table(data):\n    \"\"\"\n    Print number of unique values for each column in [data]\n    \"\"\"\n    list = []\n    for column in data.columns:\n        list.append([column, len(data[column].unique()), data[column].dtype])\n\n    table_df = pd.DataFrame(list, columns=[\"Column\", \"Number of Unique Values\", \"Dtype\"])\n    return table_df","metadata":{"execution":{"iopub.status.busy":"2023-06-14T04:20:27.511275Z","iopub.execute_input":"2023-06-14T04:20:27.512029Z","iopub.status.idle":"2023-06-14T04:20:27.519223Z","shell.execute_reply.started":"2023-06-14T04:20:27.51199Z","shell.execute_reply":"2023-06-14T04:20:27.517598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Function to print values and percentage of a column side by side","metadata":{}},{"cell_type":"code","source":"def print_side(column):\n    # Calculate the counts of each gender\n    counts = data[column].value_counts()\n\n    # Calculate the normalized percentages of each gender\n    percentages = data[column].value_counts(normalize=True) * 100\n\n    # Concatenate the two Series side by side\n    result = pd.concat([counts, percentages], axis=1)\n\n    # Rename the columns for better clarity\n    result.columns = ['Counts', 'Percentages']\n\n    # Print the result\n    print(result)","metadata":{"execution":{"iopub.status.busy":"2023-06-14T04:20:30.832615Z","iopub.execute_input":"2023-06-14T04:20:30.833421Z","iopub.status.idle":"2023-06-14T04:20:30.839539Z","shell.execute_reply.started":"2023-06-14T04:20:30.833381Z","shell.execute_reply":"2023-06-14T04:20:30.838216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extracting a list of Categorical fields with missing values\n# NOTE : Categorical fields will have data type as 'O', as can be observed in the filter condition below\n\n\nfeatures_with_missingval =[feature for feature in data.columns if data[feature].isnull().sum()>1 and data[feature].dtypes=='O']\nfeatures_with_missingval\n\n# Printing %s of missing values in all cateogrical features\n\nfor feature in features_with_missingval:\n    print(\"{0}: {1}% missing values\".format(feature,np.round(data[feature].isnull().mean()*100,2)))","metadata":{"execution":{"iopub.status.busy":"2023-05-03T12:49:52.633185Z","iopub.status.idle":"2023-05-03T12:49:52.633714Z","shell.execute_reply.started":"2023-05-03T12:49:52.633447Z","shell.execute_reply":"2023-05-03T12:49:52.633472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Get 95% of data","metadata":{}},{"cell_type":"code","source":"\nq = data['Installs'].quantile(.95)\np = data[data['Installs']>q]\ndata.drop(p.index, axis=0, inplace=True)\n(data['Installs']>q).sum()","metadata":{"execution":{"iopub.status.busy":"2023-05-03T12:49:52.635401Z","iopub.status.idle":"2023-05-03T12:49:52.635827Z","shell.execute_reply.started":"2023-05-03T12:49:52.635617Z","shell.execute_reply":"2023-05-03T12:49:52.635639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Remove conjunctions, preposition, number, nouns","metadata":{}},{"cell_type":"code","source":"# create a list of meaningful words and the number of times they appear in the given text, we can perform text analysis and count the occurrences of each word.\n\ntext = \"\"\"text\"\"\"\n\nimport re\nfrom collections import Counter\n\n# Remove punctuation and convert to lowercase\nclean_text = re.sub(r'[^\\w\\s]', '', text.lower())\n\n# Split the text into individual words\nwords = clean_text.split()\n\n# Exclude common words, numbers, and non-meaningful characters\nmeaningful_words = [word for word in words if word not in ['and', 'or', 'the', 'at', 'in', 'to', 'a', 'on', 'is', 'it'] and not bool(re.match(r'\\d+', word))]\n\n# Count the occurrences of each word\nword_counts = Counter(meaningful_words)\n\n\n# Convert the dictionary to a pandas DataFrame\ndata = pd.DataFrame.from_dict(Counter(word_counts), orient='index', columns=['count'])\n\n# Sort the DataFrame by the count column in descending order\ndata = data.sort_values('count', ascending=False)\n\n# Reset the index of the DataFrame\ndata = data.reset_index()\n\n# Rename the columns\ndata.columns = ['word', 'count']\n\n# output as a file\n#data.to_excel('text.xlsx')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Gender list","metadata":{}},{"cell_type":"code","source":"female_list = ['female','f','cis female','woman','femake','female ','cis-female/femme','female (cis)','femail']\n\nmale_list = ['m','male','male-ish','maile','cis male','mal','male (cis)','make','male ','man','msle','mail','malr','cis man']\n\ntrans_list = ['trans-female','something kinda male?','queer/she/they','genderqueer','androgyne','agender','male leaning androgynous','trans woman','female (trans)','queer','guy (-ish) ^_^']\n\nnon_binary = ['non-binary','nah','all','enby','fluid','neuter','ostensibly male, unsure what that really means']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Multicollinearity","metadata":{}},{"cell_type":"code","source":"from statsmodels.stats.outliers_influence import variance_inflation_factor\n\ndata = df.drop(['Close'], axis=1)\ntarget = df['Close']\n\n\nvif = pd.DataFrame()\nvif[\"variables\"] = data.columns\nvif[\"VIF\"] = [variance_inflation_factor(data.values, i) for i in range(data.shape[1])]\n\n# Print the VIF values\nprint(vif)\n\n# Remove features with high VIF\nthreshold = 5\nwhile max(vif[\"VIF\"]) > threshold:\n    # Get the feature with the highest VIF\n    max_vif_feature = vif.loc[vif[\"VIF\"].idxmax(), \"variables\"]\n    # Remove the feature from the dataset and VIF table\n    data = data.drop(columns=[max_vif_feature])\n    vif = vif.drop(vif[vif[\"variables\"] == max_vif_feature].index)\n    # Recalculate VIF values\n    vif[\"VIF\"] = [variance_inflation_factor(data.values, i) for i in range(data.shape[1])]\n\n# Print the remaining features\nprint(data.columns)\n\n# This code can remove the multicollinearity from your data before using this you need to split the data into independent data and target so that it doesnt remove the target","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <div style=\"text-align: left; background-color:#1192AA; font-family: Trebuchet MS; color: Yellow; padding: 13px; line-height:0.9;border-radius:2px; margin-bottom: 0em; text-align: center; font-size: 25px\">Map</div>","metadata":{}},{"cell_type":"markdown","source":"    import folium #Map lib\n    geo = data[['City', 'Longitude', 'Latitude']].drop_duplicates()\n    geo_map = folium.Map(location = [16.8409,96.1735 ], zoom_start = 5, tiles ='openstreetmap', control_scale = True)\n    for (i,j) in geo.iterrows():\n        folium.Marker(location =[j.loc['Latitude'],j.loc['Longitude']], popup = j.loc['City'], toottip = 'click',\n                      icon = folium.Icon(icon = 'glyphicon-shopping-cart', prefix ='glyphicon' )).add_to(geo_map)\n    geo_map\n    \n    different views(tiles) of maps https://deparkes.co.uk/2016/06/10/folium-map-tiles/","metadata":{}},{"cell_type":"markdown","source":"## <div style=\"text-align: left; background-color:#1192AA; font-family: Trebuchet MS; color: Yellow; padding: 13px; line-height:0.9;border-radius:2px; margin-bottom: 0em; text-align: center; font-size: 25px\">Models</div>","metadata":{}},{"cell_type":"markdown","source":"Classification and regression are two different types of supervised machine learning tasks. Classification involves predicting a category or class label, typically 1 or 0 for binary classification, while regression involves predicting a continuous numerical value. Here are some common models for both classification and regression tasks, along with their names:\n\n**Classification Models (for predicting 1 or 0):**\n\n1. **Logistic Regression:** A linear model used for binary classification. It models the probability of an example belonging to a particular class.\n\n2. **Decision Trees:** Tree-based models that partition the data into subsets based on feature values, ultimately assigning class labels to these subsets.\n\n3. **Random Forest:** An ensemble of decision trees. It combines multiple trees to improve predictive performance and reduce overfitting.\n\n4. **Support Vector Machines (SVM):** A model that finds a hyperplane that best separates the data points belonging to different classes.\n\n5. **K-Nearest Neighbors (KNN):** A simple algorithm that classifies data points based on the majority class among their k-nearest neighbors in feature space.\n\n6. **Naive Bayes:** A probabilistic model based on Bayes' theorem that is often used for text classification but can be applied to various classification tasks.\n\n7. **Gradient Boosting (e.g., XGBoost, LightGBM, CatBoost):** Ensemble methods that combine multiple weak learners to create a strong classifier. These models can handle both binary and multiclass classification.\n\n**Regression Models (for predicting continuous values):**\n\n1. **Linear Regression:** A simple linear model that fits a straight line to the data to predict continuous values.\n\n2. **Decision Trees and Regression Trees:** Similar to their classification counterparts, but used for regression tasks.\n\n3. **Random Forest Regression:** An ensemble of regression trees, used for improving the accuracy of regression tasks and reducing overfitting.\n\n4. **Support Vector Regression (SVR):** An extension of SVM for regression tasks. It tries to find a hyperplane that best fits the data.\n\n5. **K-Nearest Neighbors (KNN) Regression:** Similar to KNN classification, but used for regression by averaging or taking the weighted average of the k-nearest neighbors' target values.\n\n6. **Gradient Boosting Regressors (e.g., XGBoost, LightGBM, CatBoost):** Ensemble methods for regression tasks, similar to their classification counterparts.\n\n7. **Neural Networks:** Deep learning models can be used for both regression and classification tasks. For regression, the output layer typically has a single node with a linear activation function.\n\nThe choice of model depends on the specific problem and the nature of the data. You can experiment with multiple models and select the one that performs best for your particular task and dataset. Additionally, it's important to consider factors like data preprocessing, feature engineering, and hyperparameter tuning to optimize model performance.","metadata":{}},{"cell_type":"markdown","source":"## Lazy Predict","metadata":{}},{"cell_type":"code","source":"# Install\n!pip install lazypredict\n\n# To use Lazy Predict in a project:\nimport lazypredict\n\n# Regression\nfrom lazypredict.Supervised import LazyRegressor\nfrom sklearn import datasets\nfrom sklearn.utils import shuffle\nimport numpy as np\nboston = datasets.load_boston()\nX, y = shuffle(boston.data, boston.target, random_state=13)\nX = X.astype(np.float32)\noffset = int(X.shape[0] * 0.9)\nX_train, y_train = X[:offset], y[:offset]\nX_test, y_test = X[offset:], y[offset:]\nreg = LazyRegressor(verbose=0,ignore_warnings=False, custom_metric=None )\nmodels,predictions = reg.fit(X_train, X_test, y_train, y_test)\n\n# Classifier\nfrom lazypredict.Supervised import LazyClassifier\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\ndata = load_breast_cancer()\nX = data.data\ny= data.target\nX_train, X_test, y_train, y_test = train_test_split(X, y,test_size=.5,random_state =123)\nclf = LazyClassifier(verbose=0,ignore_warnings=True, custom_metric=None)\nmodels,predictions = clf.fit(X_train, X_test, y_train, y_test)\nmodels","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Over/Under-fiting Function ","metadata":{}},{"cell_type":"code","source":"# function calculats mean_squared_error, r2_score, accuracy_score with margin 5%\nfrom sklearn.metrics import mean_squared_error, r2_score, accuracy_score\n\ndef check_fit(X_train, y_train, X_test, y_test, model):\n    margin = 0.05\n    print('\\t\\t\\t\\t','Model Used:', model, '|  Margin for fitting:' , margin)\n    \n    # Fit the model on the training data\n    model.fit(X_train, y_train)\n\n    # Predict on training and test data\n    y_train_predict = model.predict(X_train)\n    y_test_predict = model.predict(X_test)\n\n    # Calculate the mean squared error\n    train_mse = mean_squared_error(y_train, y_train_predict)\n    test_mse = mean_squared_error(y_test, y_test_predict)\n\n    # Calculate the coefficient of determination (R-squared)\n    train_r2 = r2_score(y_train, y_train_predict)\n    test_r2 = r2_score(y_test, y_test_predict)\n    \n    # Train and Testing Accuracy\n    train_ac = accuracy_score(y_train, y_train_predict)\n    test_ac = accuracy_score(y_test, y_test_predict)\n\n    # Print the results\n    print(\"Training MSE: {:.4f}\".format(train_mse))\n    print(\"Testing MSE: {:.4f}\".format(test_mse))\n    print(\"Training R^2: {:.4f}\".format(train_r2))\n    print(\"Testing R^2: {:.4f}\".format(test_r2))\n    print(\"Training Accuracy: {:.4f}\".format(train_ac))\n    print(\"Testing Accuracy: {:.4f}\".format(test_ac))\n\n    # Compare training and testing MSE\n    print('Mean Squared Error: ',end='')\n    if train_mse < test_mse:\n        print(\"The model may be overfitting.\")\n    elif train_mse > test_mse:\n        print(\"The model may be underfitting.\")\n    else:\n        print(\"The model appears to have a good fit.\")\n\n    # Compare training and testing R-squared\n    print('R-squared: ',end='')\n    if train_r2 < test_r2:\n        print(\"The model may be overfitting.\")\n    elif train_r2 > test_r2:\n        print(\"The model may be underfitting.\")\n    else:\n        print(\"The model appears to have a good fit.\")\n        \n    # Compare training and testing Accuracy Score\n    print('Accuracy Score: ',end='')\n    if train_ac-margin > test_ac:\n        print(\"The model may be overfitting.\")\n    elif train_ac < test_ac-margin:\n        print(\"The model may be underfitting.\")\n    else:\n        print(\"The model appears to have a good fit.\")\n    ","metadata":{"execution":{"iopub.status.busy":"2023-06-14T03:21:25.776611Z","iopub.execute_input":"2023-06-14T03:21:25.777042Z","iopub.status.idle":"2023-06-14T03:21:26.251893Z","shell.execute_reply.started":"2023-06-14T03:21:25.777004Z","shell.execute_reply":"2023-06-14T03:21:26.250109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Regressions (ChartGPU)","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.tree import DecisionTreeRegressor\n\n# Create a list of regression models\nmodels = [\n    RandomForestRegressor(),\n    GradientBoostingRegressor(),\n    LinearRegression(),\n    Ridge(),\n    Lasso(),\n    KNeighborsRegressor(),\n    SVR(),\n    DecisionTreeRegressor()\n]\n\nresults = []\n\n# Loop through models and perform cross-validation\nfor model in models:\n    scores = cross_val_score(model, X, y, cv=5, n_jobs=-1, scoring='accuracy')\n    mean_score = scores.mean()\n    \n    results.append({'Model': model.__class__.__name__, 'Accuracy': mean_score})\n\n# Create a DataFrame to display results\nresults_df = pd.DataFrame(results)\nresults_df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Hyperparameter Regression (ChatGPU)","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.tree import DecisionTreeRegressor\n\n# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Define a list of regressors with their hyperparameter grids\nregressors = [\n    {\n        'model': RandomForestRegressor(),\n        'param_grid': {\n            'n_estimators': [100, 200],\n            'max_depth': [None, 5, 10],\n            'min_samples_split': [2, 5, 10]\n        }\n    },\n    {\n        'model': GradientBoostingRegressor(),\n        'param_grid': {\n            'n_estimators': [100, 200],\n            'learning_rate': [0.01, 0.1],\n            'max_depth': [3, 5]\n        }\n    },\n    {\n        'model': LinearRegression(),\n        'param_grid': {}\n    },\n    {\n        'model': KNeighborsRegressor(),\n        'param_grid': {\n            'n_neighbors': [3, 5, 7]\n        }\n    },\n    {\n        'model': SVR(),\n        'param_grid': {\n            'C': [1, 10, 100],\n            'kernel': ['linear', 'rbf']\n        }\n    },\n    {\n        'model': DecisionTreeRegressor(),\n        'param_grid': {\n            'max_depth': [None, 5, 10],\n            'min_samples_split': [2, 5, 10]\n        }\n    }\n]\n\n# Loop through regressors and perform GridSearchCV\nfor regressor in regressors:\n    model = regressor['model']\n    param_grid = regressor['param_grid']\n    \n    grid_search = GridSearchCV(model, param_grid, scoring='accuracy', n_jobs=-1, cv=5)\n    grid_search.fit(X_train, y_train)\n    \n    best_params = grid_search.best_params_\n    best_score = grid_search.best_score_\n    \n    y_pred = grid_search.predict(X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n    \n    print(f\"Model: {model.__class__.__name__}\")\n    print(f\"Best Hyperparameters: {best_params}\")\n    print(f\"Best Cross-Validation Accuracy: {best_score:.4f}\")\n    print(f\"Test Accuracy: {accuracy:.4f}\\n\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Hyperparameter Classifiers","metadata":{}},{"cell_type":"code","source":"random_state = 42\nclassifier = [DecisionTreeClassifier(random_state = random_state),\n             SVC(random_state = random_state),\n             RandomForestClassifier(random_state = random_state),\n             LogisticRegression(random_state = random_state),\n             KNeighborsClassifier()]\n\ndt_param_grid = {\"min_samples_split\" : range(10,500,20),\n                \"max_depth\": range(1,20,2)}\n\nsvc_param_grid = {\"kernel\" : [\"rbf\"],\n                 \"gamma\": [0.001, 0.01, 0.1, 1],\n                 \"C\": [1,10,50,100,200,300,1000]}\n\nrf_param_grid = {\"max_features\": [1,3,10],\n                \"min_samples_split\":[2,3,10],\n                \"min_samples_leaf\":[1,3,10],\n                \"bootstrap\":[False],\n                \"n_estimators\":[100,300],\n                \"criterion\":[\"gini\"]}\n\nlogreg_param_grid = {\"C\":np.logspace(-3,3,7),\n                    \"penalty\": [\"l2\"], 'solver':['lbfgs'], 'max_iter':[3000]}\n\nknn_param_grid = {\"n_neighbors\": np.linspace(1,19,10, dtype = int).tolist(),\n                 \"weights\": [\"uniform\",\"distance\"],\n                 \"metric\":[\"euclidean\",\"manhattan\"]}\nclassifier_param = [dt_param_grid,\n                   svc_param_grid,\n                   rf_param_grid,\n                   logreg_param_grid,\n                   knn_param_grid]\ncv_result = []\nbest_estimators = []\nfor i in range(len(classifier)):\n    clf = GridSearchCV(classifier[i], param_grid=classifier_param[i], cv = StratifiedKFold(n_splits = 10), scoring = \"accuracy\", n_jobs = -1,verbose = 1)\n    clf.fit(X_train,y_train)\n    cv_result.append(clf.best_score_)\n    best_estimators.append(clf.best_estimator_)\n    print(cv_result[i])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Hyperparameters GridSearchCV","metadata":{}},{"cell_type":"code","source":"# a grid of hyperparameters and automatically search for the best parameter combination.\nfrom sklearn.model_selection import GridSearchCV\n\n# Define the hyperparameter grid\nparam_grid = {\n    'param1': [value1, value2, ...],\n    'param2': [value1, value2, ...],\n    ...\n}\n\n# Create the GridSearchCV object\ngrid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5)\n\n# Fit the data to perform the grid search\ngrid_search.fit(X, y)\n\n# Get the best parameters and best score\nbest_params = grid_search.best_params_\nbest_score = grid_search.best_score_\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Interactive Function for Elbow Method Plot and User-Defined Clusters in K-means Scatterplot ","metadata":{}},{"cell_type":"code","source":"def kmean_scatter(data):\n    # Initialize an empty list to store the within-cluster sum of squares (inertia) values\n    inertias = []\n\n    # Define a range of n_clusters values to try\n    n_clusters_range = range(1, 10)\n\n    # Iterate over the range of n_clusters values\n    for n_clusters in n_clusters_range:\n        kmeans = KMeans(n_clusters=n_clusters, init='k-means++', n_init='auto', max_iter=300, tol=0.0001, verbose=0,\n                        random_state=None, copy_x=True, algorithm='lloyd')\n        kmeans.fit(data)\n        inertias.append(kmeans.inertia_)\n\n    # Plot the within-cluster sum of squares values\n    plt.plot(n_clusters_range, inertias)\n    plt.xlabel('Number of Clusters')\n    plt.ylabel('Within-Cluster Sum of Squares')\n    plt.title('Elbow Method')\n    plt.show()\n\n    # Ask the user for the number of clusters\n    number_clusters = int(input('How many clusters: '))\n\n    # Perform k-means clustering with the chosen number of clusters\n    kmeans = KMeans(n_clusters=number_clusters, init='k-means++', n_init='auto', max_iter=300, tol=0.0001, verbose=0,\n                    random_state=None, copy_x=True, algorithm='lloyd').fit(data)\n\n    # Plot the scatter plot of the data points colored by cluster labels\n    sns.scatterplot(data, x=data[:,0], y=data[:,1], hue=kmeans.labels_, legend=False, palette='colorblind')\n\n    # Plot the centroids of the clusters\n    plt.scatter(kmeans.cluster_centers_[:,0], kmeans.cluster_centers_[:,1], color='Yellow', s=100, label='Centroids')\n\n    plt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Display Plot Tree of SKlearn Decision Trees (DTs)","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import export_graphviz\nimport pydotplus\nfrom IPython.display import Image\n\n# Generate a pydot file\ndot_tree = export_graphviz(decision_tree=model,\n                           filled=True,\n                           rounded=True,\n                           special_characters=True,\n                           feature_names=X.columns)\n\n# Generate graph using pydot file\ngraph = pydotplus.graph_from_dot_data(dot_tree)\n\n# Display the graph\nImage(graph.create_png())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluation Matrix","metadata":{}},{"cell_type":"markdown","source":"## Classification","metadata":{}},{"cell_type":"code","source":"import sklearn.metrics\n\n# Accuracy classification score\naccuracy = sklearn.metrics.accuracy_score(y_test, model.predict(X_test))\n# Area Under the Curve (AUC)\nfpr, tpr, thresholds = sklearn.metrics.roc_curve(y_test, model.predict(X_test))\nauc = sklearn.metrics.auc(fpr, tpr)\n# Average precision (AP) from prediction scores\nap = sklearn.metrics.average_precision_score(y_test, model.predict(X_test))\n# Balanced accuracy\nbalanced_accuracy = sklearn.metrics.balanced_accuracy_score(y_test, model.predict(X_test))\n# Brier score\nbrier_score = sklearn.metrics.brier_score_loss(y_test, model.predict(X_test))\n# Classification report\nclassification_report = sklearn.metrics.classification_report(y_test, model.predict(X_test))\n# Cohen's kappa\ncohen_kappa = sklearn.metrics.cohen_kappa_score(y_test, model.predict(X_test))\n# Confusion matrix\nconfusion_matrix = sklearn.metrics.confusion_matrix(y_test, model.predict(X_test))\n# F1 score / Balanced F-score or F-measure\nf1_score = sklearn.metrics.f1_score(y_test, model.predict(X_test))\n# F-beta score\nfbeta_score = sklearn.metrics.fbeta_score(y_test, model.predict(X_test), beta=0.5)\n# Hamming loss\nhamming_loss = sklearn.metrics.hamming_loss(y_test, model.predict(X_test))\n# Average hinge loss\nhinge_loss = sklearn.metrics.hinge_loss(y_test, model.predict(X_test))\n# Jaccard similarity coefficient score\njaccard_score = sklearn.metrics.jaccard_score(y_test, model.predict(X_test))\n# Log loss / logistic Loss / Cross-Entropy Loss\nlog_loss = sklearn.metrics.log_loss(y_test, model.predict(X_test))\n# Matthews correlation coefficient (MCC)\nmcc = sklearn.metrics.matthews_corrcoef(y_test, model.predict(X_test))\n# Precision-Recall pairs for different probability thresholds\nprecision_recall_curve = sklearn.metrics.precision_recall_curve(y_test, model.predict(X_test))\n# Precision, Recall, F-measure and Support\nprecision_recall_fscore_support = sklearn.metrics.precision_recall_fscore_support(y_test, model.predict(X_test))\n# Precision\nprecision = sklearn.metrics.precision_score(y_test, model.predict(X_test))\n# Recall\nrecall = sklearn.metrics.recall_score(y_test, model.predict(X_test))\n# Area Under the Receiver Operating Characteristic Curve (ROC AUC)\nroc_auc = sklearn.metrics.roc_auc_score(y_test, model.predict(X_test))\n# Receiver operating Characteristic (ROC)\nroc_curve = sklearn.metrics.roc_curve(y_test, model.predict(X_test))\n# Zero-one classification loss\nzero_one_loss = sklearn.metrics.zero_one_loss(y_test, model.predict(X_test))\n\n# Print the evaluation metrics\nprint(\"Accuracy Score:\", accuracy)\nprint(\"Area Under the Curve (AUC):\", auc)\nprint(\"Average Precision (AP):\", ap)\nprint(\"Balanced Accuracy:\", balanced_accuracy)\nprint(\"Brier Score:\", brier_score)\nprint(\"Classification Report:\")\nprint(classification_report)\nprint(\"Cohen's Kappa:\", cohen_kappa)\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix)\nprint(\"F1 Score:\", f1_score)\nprint(\"F-beta Score:\", fbeta_score)\nprint(\"Hamming Loss:\", hamming_loss)\nprint(\"Average Hinge Loss:\", hinge_loss)\nprint(\"Jaccard Similarity Coefficient Score:\", jaccard_score)\nprint(\"Log Loss:\", log_loss)\nprint(\"Matthews Correlation Coefficient (MCC):\", mcc)\nprint(\"Precision-Recall Curve:\", precision_recall_curve)\nprint(\"Precision, Recall, F-measure, Support:\", precision_recall_fscore_support)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\nprint(\"ROC AUC:\", roc_auc)\nprint(\"ROC Curve:\", roc_curve)\nprint(\"Zero-One Classification Loss:\", zero_one_loss)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Confusion Matrix Function","metadata":{}},{"cell_type":"code","source":"def confusion(X_test, y_test):\n    y_pred = model.predict(X_test)\n    cm = sklearn.metrics.confusion_matrix(y_test, y_pred)\n\n    # Format the confusion matrix using tabulate\n    table = tabulate(cm, headers=['Predicted: No', 'Predicted: Yes'], showindex=['Actual: No', 'Actual: Yes'], tablefmt='grid')\n\n    print(\"Confusion Matrix:\")\n    print(table)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Boosting","metadata":{}},{"cell_type":"markdown","source":"## ADABoost","metadata":{}},{"cell_type":"code","source":"estimators_list = list(range(1, 50, 3))\n\nscore_list = []\n\nfor n in estimators_list:\n    boost = AdaBoostClassifier(estimator=model, n_estimators=n, random_state=101)\n    boost.fit(X_train, y_train)\n    y_pred_boosted = boost.predict(X_test)\n    score_boosted = accuracy_score(y_test, y_pred_boosted)\n    score_list.append(score_boosted)\nestimators_list,score_list\n\nplt.figure(figsize=(20,5))\nplt.plot(estimators_list, score_list, label='Test')\nplt.ylabel('accuracy')\nplt.xlabel('Number of estimors')\nplt.grid()\nplt.legend()\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# This should display tree in plotly(not working)","metadata":{}},{"cell_type":"code","source":"#https://community.plotly.com/t/decision-tree-plot-plot-tree/68403\n\n!pip install pydotplus\nimport plotly.graph_objects as go\nfrom sklearn.tree import export_graphviz\nimport pydotplus\nfrom IPython.display import Image\nfrom sklearn.datasets import load_iris\nfrom sklearn import tree\nimport re\nimport numpy as np\n\n# Load the iris dataset\niris = load_iris()\nX, y = iris.data, iris.target\n\n# Train a decision tree classifier\nclf = tree.DecisionTreeClassifier()\nclf = clf.fit(X, y)\n\nmatrix = clf\n\n# Generate a pydot file\ndot_tree = export_graphviz(decision_tree=matrix,\n                           filled=True,\n                           rounded=True,\n                           special_characters=True,\n                           feature_names=iris.feature_names\n                          )\n\n# Generate graph using pydot file\ngraph = pydotplus.graph_from_dot_data(dot_tree)\n\ndef get_plotly_data(E, coords):\n    # E is the list of tuples representing the graph edges\n    # coords is the list of node coordinates\n    N = len(coords)\n    Xnodes = [coords[int(k)][0] for k in range(N)]  # x-coordinates of nodes\n    Ynodes = [coords[int(k)][1] for k in range(N)]  # y-coordinates of nodes\n\n    Xedges = []\n    Yedges = []\n    for e in E:\n        src, dest = int(e[0]), int(e[1])\n        Xedges.extend([coords[src][0], coords[dest][0], None])\n        Yedges.extend([coords[src][1], coords[dest][1], None])\n\n    return Xnodes, Ynodes, Xedges, Yedges\n\n\ndef get_node_trace(x, y, labels, marker_size=5, marker_color='#6959CD', \n                   line_color='rgb(50,50,50)', line_width=0.5):\n    return go.Scatter(\n        x=x,\n        y=y,\n        mode='markers',\n        marker=dict(\n            size=marker_size, \n            color=marker_color,\n            line=dict(color=line_color, width=line_width)\n        ),\n        text=labels,\n        hoverinfo='text'\n    )\n\ndef get_edge_trace(x, y, line_color='rgb(210,210,210)', line_width=1):\n    return go.Scatter(\n        x=x,\n        y=y,\n        mode='lines',\n        line_color=line_color,\n        line_width=line_width,\n        hoverinfo='none'\n    )\n\n# Extract node coordinates from the plot_tree output\nt = tree.plot_tree(matrix)\ncoords = []\nfor text in t:\n    pattern = r\"Annotation\\((\\d+\\.\\d+), (\\d+\\.\\d+)\"\n    matches = re.search(pattern, str(text))\n    if matches:\n        x_coordinate = float(matches.group(1))\n        y_coordinate = float(matches.group(2))\n        coords.append([x_coordinate, y_coordinate])\ncoords = np.array(coords)\n\n# Extract edges from the pydot graph\nE = []\nfor edge in graph.get_edges():\n    src, dest = edge.get_source(), edge.get_destination()\n    E.append((src, dest))\nlabels= np.arange(17)\n# Get the Plotly data\nXnodes, Ynodes, Xedges, Yedges = get_plotly_data(E, coords)\nnodes = get_node_trace(Xnodes, Ynodes, labels, marker_size=18, marker_color='#c0c0c0')\nedges = get_edge_trace(Xedges, Yedges)\n\n# Create the figure and update layout\nfig = go.Figure([edges, nodes])\nfig.update_layout(\n    title_text=\"Decision tree\",\n    title_x=0.5,\n    font_size=12,\n    showlegend=False,\n    width=800,\n    height=600,\n    xaxis_visible=False,\n    yaxis_visible=False, \n    template='none',\n    hovermode='closest',\n    paper_bgcolor='#eeeeee'\n)\n\nfig.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-05-23T18:14:02.473114Z","iopub.execute_input":"2023-05-23T18:14:02.473643Z","iopub.status.idle":"2023-05-23T18:14:24.56521Z","shell.execute_reply.started":"2023-05-23T18:14:02.473598Z","shell.execute_reply":"2023-05-23T18:14:24.563201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create a pivot from scratch\nid_start_value = df['id_start'].iloc[0] #1001400\nid_end_value = df['id_end'].iloc[-1] #1001472\nindex = list(df['id_start'])\ncolumn = list(df['id_end'])\nindex.append(id_end_value)\ncolumn.insert(0,id_start_value)\n\ndf_pivot = pd.DataFrame(0, index=new_index, columns=column)","metadata":{"execution":{"iopub.status.busy":"2023-12-09T21:37:20.486096Z","iopub.execute_input":"2023-12-09T21:37:20.486998Z","iopub.status.idle":"2023-12-09T21:37:20.573398Z","shell.execute_reply.started":"2023-12-09T21:37:20.486955Z","shell.execute_reply":"2023-12-09T21:37:20.571495Z"},"trusted":true},"execution_count":null,"outputs":[]}]}